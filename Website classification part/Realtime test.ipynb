{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import whois\n",
    "import virustotal_python\n",
    "from requests_html import HTMLSession\n",
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "from urllib3 import disable_warnings\n",
    "\n",
    "disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    " \n",
    "from collections import defaultdict\n",
    "from urllib.request import urlopen, URLError\n",
    "import socket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'reddit.com'\n",
    "\n",
    "def get_pagerank(domain):\n",
    "    headers = {'API-OPR':'4ok88wgckg8o0cgcswo4gc4kkc0sgocw0woww4o0'}\n",
    "    url = 'https://openpagerank.com/api/v1.0/getPageRank?domains%5B0%5D=' + domain\n",
    "    request = requests.get(url, headers=headers)\n",
    "    if request.status_code == 200:\n",
    "        result = request.json()\n",
    "        if result['status_code'] == 200:\n",
    "            pr_dec = result['response'][0]['page_rank_decimal']\n",
    "            rank = result['response'][0]['rank']\n",
    "\n",
    "            return pr_dec, rank\n",
    "\n",
    "def whois_checker(domain):\n",
    "    w = whois.whois(domain)\n",
    "    domain_registrar = w.get('registrar')\n",
    "    postal_code = w.get('registrant_postal_code')\n",
    "    country = w.get('country')\n",
    "\n",
    "    return  domain_registrar, postal_code, country\n",
    "\n",
    "def virus_scan(domain):\n",
    "    with virustotal_python.Virustotal(\"242305dedab935c6a39736d0749ae88f2b68f1e2f0c8e82c81d8955c5fd194d3\") as vtotal:\n",
    "        resp = vtotal.request(f\"domains/{domain}\")\n",
    "        virus_data = resp.json()['data']['attributes']['last_analysis_stats']\n",
    "        return virus_data['malicious']\n",
    "\n",
    "\n",
    "def link_checker(domain):\n",
    "    session = HTMLSession()\n",
    "    \n",
    "    r = session.get(\"https://\"+domain, verify=False)\n",
    "    unique_netlocs = Counter(urlparse(link).netloc for link in r.html.absolute_links)\n",
    "    outbound_n = 0\n",
    "    local_n = 0\n",
    "\n",
    "    for link in unique_netlocs:\n",
    "        # print(link, unique_netlocs[link])\n",
    "        if domain.lower() in link:\n",
    "            local_n += unique_netlocs[link]\n",
    "        else:\n",
    "            outbound_n += unique_netlocs[link]\n",
    "\n",
    "    return local_n, local_n + outbound_n\n",
    "\n",
    "\n",
    "\n",
    "def check_login(domain, driver):\n",
    "    wp_xpath = \"//a[starts-with(@href, 'https://wordpress.org')]\"\n",
    "    try:\n",
    "        displayed = driver.find_element('xpath', wp_xpath).is_displayed()\n",
    "\n",
    "        return displayed\n",
    "    except:\n",
    "\n",
    "        return False\n",
    "\n",
    "def check_page_source(domain, driver):\n",
    "    try:\n",
    "        driver.get(f\"https://{domain}/wp-admin\")\n",
    "        wp_pg = \"wp-content\" in driver.page_source\n",
    "        return wp_pg\n",
    "    except:\n",
    "\n",
    "        return False\n",
    "\n",
    "def check_wp(domain):\n",
    "\n",
    "    service_obj = Service(executable_path=\"C:/Users/cvaal/chromedriver.exe\")\n",
    "    driver = webdriver.Chrome(options=options, service=service_obj)\n",
    "    driver.implicitly_wait(1)\n",
    "    driver.get(f\"https://{domain}/wp-admin\")\n",
    "    login = check_login(domain, driver)\n",
    "    pg_source = check_page_source(domain, driver)\n",
    "    wp = None\n",
    "    if login == False and pg_source == False:\n",
    "        wp = False\n",
    "    else:\n",
    "        wp = True\n",
    "\n",
    "    driver.close()\n",
    "    return wp\n",
    "\n",
    "\n",
    "def check_url( url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36'}\n",
    "    response = requests.get('https://'+url, timeout=5, headers=headers)\n",
    "   \n",
    "    return response.status_code\n",
    "\n",
    "\n",
    "def get_site_data(domain):\n",
    "    # try:\n",
    "    # print(check_url(domain))\n",
    "    # if check_url(domain) == True:\n",
    "    # try:\n",
    "    pr_dec, rank = get_pagerank(domain)\n",
    "    domain_registrar, postal_code, country = whois_checker(domain)\n",
    "    malicious = virus_scan(domain)\n",
    "    local_n, total_links= link_checker(domain)\n",
    "    wp_check = check_wp(domain)\n",
    "    return domain, pr_dec, rank, domain_registrar, postal_code, country, malicious, local_n, total_links, wp_check\n",
    "# except:\n",
    "    #     return('site down')\n",
    "    # else:\n",
    "    #     return 'Site down'\n",
    "    # # except:\n",
    "    # #     return 'Site down'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FakeNews_URLs.txt', 'r') as f:\n",
    "    fakenews_sites = f.read().split('\\n')\n",
    "    fakenews_sites = [site.strip() for site in fakenews_sites]\n",
    "\n",
    "\n",
    "def get_and_organize_data(domain):\n",
    "    columns = ['Domain', 'Page rank decimal', 'Site Rank', 'Domain registrar', 'Postal code', 'Country of origin', 'Malicious', 'No. of Local links','Total links','Wordpress?']\n",
    "    site_df_dict = defaultdict(list)\n",
    "    site_data = get_site_data(domain.lower())\n",
    "    print(site_data)\n",
    "    if type(site_data) == tuple:\n",
    "        for column, data in zip(columns, site_data):\n",
    "            site_df_dict[column].append(data)\n",
    "         \n",
    "        return pd.DataFrame(site_df_dict)\n",
    "    else:\n",
    "        print('site down')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "\n",
    "ct = joblib.load('SitePreprocessor.joblib')\n",
    "clf = joblib.load('XGBoostClassifier.joblib')\n",
    "\n",
    "def predict(domain):\n",
    "    if 'http' in domain:\n",
    "        domain = urlparse(domain).netloc.strip(\"www.\")\n",
    "    \n",
    "    print('scanning:' + domain)\n",
    "    data = get_and_organize_data(domain) #dataframe obj\n",
    "    domains = data['Domain'].values\n",
    "    features_df = data.drop('Domain', axis=1)\n",
    "    \n",
    "\n",
    "    result = clf.predict(features_df)\n",
    "    if result > 0.5:\n",
    "        print(f'{domain} : reliable site')\n",
    "    else:\n",
    "        print(f'{domain} : Unreliable site')\n",
    "\n",
    "#idea: create instance of organizing object everytime the system is ran, collects all the links and returns a full dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FilipinoRealNews.txt', 'r') as f:\n",
    "    fil_real_sites = f.read().split('\\n')\n",
    "    fil_real_sites= [site.strip() for site in fil_real_sites]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning:businessmirror.com.ph\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "('businessmirror.com.ph', 4.64, '50443', None, None, None, 0, 479, 493, True)\n",
      "businessmirror.com.ph : Unreliable site\n",
      "scanning:bworldonline.com\n",
      "('bworldonline.com', 5.06, '18162', 'Network Solutions, LLC', '1112', 'PH', 0, 146, 158, True)\n",
      "bworldonline.com : reliable site\n",
      "scanning:tribune.net.ph\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "('tribune.net.ph', 4.61, '55796', None, None, None, 0, 152, 160, True)\n",
      "tribune.net.ph : Unreliable site\n",
      "scanning:malaya.com.ph\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "('malaya.com.ph', 4.28, '274928', None, None, None, 0, 264, 269, True)\n",
      "malaya.com.ph : Unreliable site\n",
      "scanning:mb.com.ph\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "('mb.com.ph', 4.8, '30852', None, None, None, 0, 169, 194, True)\n",
      "mb.com.ph : reliable site\n",
      "scanning:manilastandard.net\n",
      "('manilastandard.net', 4.51, '83092', 'DomainPeople, Inc.', None, None, 0, 326, 337, True)\n",
      "manilastandard.net : Unreliable site\n",
      "scanning:manilatimes.net\n",
      "('manilatimes.net', 4.96, '21625', 'Network Solutions, LLC', '32256', 'US', 0, 71, 89, False)\n",
      "manilatimes.net : reliable site\n",
      "scanning:journal.com.ph\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "('journal.com.ph', 4.25, '374197', None, None, None, 0, 124, 127, False)\n",
      "journal.com.ph : reliable site\n",
      "scanning:philstar.com\n",
      "('philstar.com', 5.17, '15676', 'Network Solutions, LLC', '32256', 'US', 0, 184, 252, False)\n",
      "philstar.com : reliable site\n"
     ]
    }
   ],
   "source": [
    "for site in fil_real_sites:\n",
    "    predict(site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning:officialgazette.gov.ph\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "('officialgazette.gov.ph', 4.58, '62421', None, None, None, 0, 59, 72, False)\n",
      "officialgazette.gov.ph : Unreliable site\n"
     ]
    }
   ],
   "source": [
    "predict('https://www.officialgazette.gov.ph/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-93084fb2a806>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msite_df_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "site_df_dict = defaultdict(list)\n",
    "\n",
    "for rating, domains in site_dict.items():\n",
    "    for i, domain in enumerate(domains):\n",
    "        print(f'Analyzing Site {i+1}: {domain} of rating: {rating}')\n",
    "        site_data = get_site_data(domain.lower(), rating)\n",
    "        if type(site_data) == tuple:\n",
    "            for column, data in zip(columns, site_data):\n",
    "                if type(data) != list:\n",
    "                    site_df_dict[column].append(data)\n",
    "                else:\n",
    "                    if any(site in data for site in fakenews_sites):\n",
    "                        site_df_dict[column].append(True)\n",
    "                    else:\n",
    "                        site_df_dict[column].append(False)\n",
    "            print('Data Gathered')\n",
    "        else:\n",
    "            print('site down')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a51e5855e04901d00966621b96dcfa06fb6a1bfd2a37e82f444787cca980996"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
